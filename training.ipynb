{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-10T22:43:05.129258Z","iopub.status.busy":"2022-02-10T22:43:05.128627Z","iopub.status.idle":"2022-02-10T22:43:10.257023Z","shell.execute_reply":"2022-02-10T22:43:10.256106Z","shell.execute_reply.started":"2022-02-10T22:43:05.129132Z"},"trusted":true},"outputs":[],"source":["import os\n","import cv2\n","import glob\n","import time\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","from ast import literal_eval\n","from IPython import display\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","tf.config.run_functions_eagerly(True)\n","tf.__version__\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-10T22:43:10.261362Z","iopub.status.busy":"2022-02-10T22:43:10.260915Z","iopub.status.idle":"2022-02-10T22:43:10.272839Z","shell.execute_reply":"2022-02-10T22:43:10.272158Z","shell.execute_reply.started":"2022-02-10T22:43:10.261324Z"},"trusted":true},"outputs":[],"source":["def extract_iris_rectangle(image_dir, points, resize=False, size=(256, 256)):\n","    image = cv2.imread(image_dir)\n","    x, y, w, h = cv2.boundingRect(points)\n","    margin = 1\n","    crop = image[y-margin:y+h+margin, x-margin:x+w+margin]\n","    if resize:\n","        crop = cv2.resize(crop, size)\n","    return crop\n","\n","\n","def save_image(img, dir):\n","    cv2.imwrite(dir, img)\n","\n","\n","def convert_string_to_np_array(value):\n","    ptsStr = literal_eval(value)\n","    return np.array(ptsStr, np.int32)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-10T22:43:10.279449Z","iopub.status.busy":"2022-02-10T22:43:10.277223Z","iopub.status.idle":"2022-02-10T22:43:10.288005Z","shell.execute_reply":"2022-02-10T22:43:10.287181Z","shell.execute_reply.started":"2022-02-10T22:43:10.279405Z"},"trusted":true},"outputs":[],"source":["def create_dataset(df, data_dir, output_dir):\n","    print(f'Number of images: {len(df)}')\n","    os.mkdir(output_dir)\n","    for _, row in df.iterrows():\n","        try:\n","            file = os.path.join(data_dir, str(row[\"identity\"]), row[\"file\"])\n","            dest = os.path.join(output_dir, row[\"file\"])\n","            points = convert_string_to_np_array(row[\"outerPts\"])\n","            iris = extract_iris_rectangle(file, points)\n","            save_image(iris, dest)\n","        except Exception as e:\n","            continue\n","    print(f'Number of processed images: {len(glob.glob(output_dir+\"/*\"))}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-10T22:43:10.290824Z","iopub.status.busy":"2022-02-10T22:43:10.290285Z","iopub.status.idle":"2022-02-10T22:43:10.297819Z","shell.execute_reply":"2022-02-10T22:43:10.296510Z","shell.execute_reply.started":"2022-02-10T22:43:10.290788Z"},"trusted":true},"outputs":[],"source":["def get_dataset(datapath, image_size, batch_size):\n","    dataset = keras.preprocessing.image_dataset_from_directory(\n","        datapath, label_mode=None, image_size=image_size, batch_size=batch_size, color_mode='rgb')\n","    dataset = dataset.shuffle(len(dataset))\n","    dataset = dataset.map(lambda x: (x - 127.5) / 127.5)\n","    return dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-10T22:43:10.299694Z","iopub.status.busy":"2022-02-10T22:43:10.299296Z","iopub.status.idle":"2022-02-10T22:43:10.353985Z","shell.execute_reply":"2022-02-10T22:43:10.353074Z","shell.execute_reply.started":"2022-02-10T22:43:10.299661Z"},"trusted":true},"outputs":[],"source":["class GAN():\n","    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","    def __init__(self, latent_dim, input_shape, batch_size, l_rate):\n","        self.latent_dim = latent_dim\n","        self.input_shape = input_shape\n","        self.batch_size = batch_size\n","        self.l_rate = l_rate\n","        self.d_losses = []\n","        self.g_losses = []\n","        self.generator_optimizer = tf.keras.optimizers.Adam(l_rate)\n","        self.discriminator_optimizer = tf.keras.optimizers.Adam(l_rate)\n","        self.generator = self.make_generator_model()\n","        self.discriminator = self.make_discriminator_model()\n","\n","    def make_generator_model(self):\n","        model = tf.keras.Sequential()\n","        model.add(layers.Dense(8*8*512, use_bias=False,\n","                  input_shape=(self.latent_dim,)))\n","        model.add(layers.Reshape((8, 8, 512)))\n","        model.add(layers.LeakyReLU())\n","\n","        model.add(layers.Conv2DTranspose(\n","            256, (4, 4), strides=(2, 2), padding='same'))\n","        model.add(layers.BatchNormalization())\n","        model.add(layers.LeakyReLU())\n","\n","        model.add(layers.Conv2DTranspose(\n","            128, (4, 4), strides=(2, 2), padding='same'))\n","        model.add(layers.BatchNormalization())\n","        model.add(layers.LeakyReLU())\n","\n","        model.add(layers.Conv2DTranspose(\n","            128, (4, 4), strides=(2, 2), padding='same'))\n","        model.add(layers.BatchNormalization())\n","        model.add(layers.LeakyReLU())\n","\n","        model.add(layers.Conv2DTranspose(3, (4, 4), strides=(\n","            2, 2), padding='same', activation='tanh'))\n","\n","        return model\n","\n","    def test_generator(self):\n","        noise = tf.random.normal([1, self.latent_dim])\n","        generated_image = self.generator(noise, training=False)\n","        plt.imshow(\n","            np.array(generated_image[0] * 127.5 + 127.5).astype(np.uint8))\n","\n","    def make_discriminator_model(self):\n","        model = tf.keras.Sequential()\n","        model.add(layers.Conv2D(64, (4, 4), strides=(2, 2),\n","                  padding='same', input_shape=self.input_shape))\n","        model.add(layers.BatchNormalization())\n","        model.add(layers.LeakyReLU())\n","\n","        model.add(layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same'))\n","        model.add(layers.BatchNormalization())\n","        model.add(layers.LeakyReLU())\n","\n","        model.add(layers.Conv2D(256, (4, 4), strides=(2, 2), padding='same'))\n","        model.add(layers.BatchNormalization())\n","        model.add(layers.LeakyReLU())\n","\n","        model.add(layers.Conv2D(512, (4, 4), strides=(2, 2), padding='same'))\n","        model.add(layers.BatchNormalization())\n","        model.add(layers.LeakyReLU())\n","\n","        model.add(layers.Flatten())\n","        model.add(layers.Dropout(0.4))\n","\n","        model.add(layers.Dense(1, activation=\"sigmoid\"))\n","\n","        return model\n","\n","    def test_discriminator(self):\n","        noise = tf.random.normal([1, self.latent_dim])\n","        generated_image = self.generator(noise, training=False)\n","        return self.discriminator(generated_image)\n","\n","    def discriminator_loss(self, real_output, fake_output):\n","        real_loss = self.cross_entropy(tf.ones_like(real_output), real_output)\n","        fake_loss = self.cross_entropy(tf.zeros_like(fake_output), fake_output)\n","        total_loss = real_loss + fake_loss\n","        return total_loss\n","\n","    def generator_loss(self, fake_output):\n","        return self.cross_entropy(tf.ones_like(fake_output), fake_output)\n","\n","    def train(self, dataset, epochs):\n","        for epoch in range(epochs):\n","            start = time.time()\n","\n","            for image_batch in dataset:\n","                self.train_step(image_batch)\n","            display.clear_output(wait=True)\n","            seed = tf.random.normal([16, self.latent_dim])\n","            self.show_generated_images(epoch + 1, seed)\n","\n","            print('Time for epoch {} is {} sec'.format(\n","                epoch + 1, time.time()-start))\n","        display.clear_output(wait=True)\n","        img = self.show_generated_images(epochs, seed)\n","\n","    @tf.function\n","    def train_step(self, images):\n","        noise = tf.random.normal([self.batch_size, self.latent_dim])\n","\n","        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","            generated_images = self.generator(noise, training=True)\n","\n","            real_output = self.discriminator(images, training=True)\n","            fake_output = self.discriminator(generated_images, training=True)\n","\n","            gen_loss = self.generator_loss(fake_output)\n","            disc_loss = self.discriminator_loss(real_output, fake_output)\n","            d_l = disc_loss.numpy()\n","            g_l = gen_loss.numpy()\n","            self.d_losses.append(d_l)\n","            self.g_losses.append(g_l)\n","            print(f'\\rd_loss: {d_l}, g_loss: {g_l}', end=\"\")\n","\n","        gradients_of_generator = gen_tape.gradient(\n","            gen_loss, self.generator.trainable_variables)\n","        gradients_of_discriminator = disc_tape.gradient(\n","            disc_loss, self.discriminator.trainable_variables)\n","\n","        self.generator_optimizer.apply_gradients(\n","            zip(gradients_of_generator, self.generator.trainable_variables))\n","        self.discriminator_optimizer.apply_gradients(\n","            zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n","\n","    def show_generated_images(self, epoch, test_input):\n","        predictions = self.generator(test_input, training=False)\n","        fig = plt.figure(figsize=(4, 4))\n","        for i in range(predictions.shape[0]):\n","            plt.subplot(4, 4, i+1)\n","            img = np.array(predictions[i] * 127.5 + 127.5).astype(np.uint8)\n","            plt.imshow(img)\n","            plt.axis('off')\n","        print(f'\\r', end=\"\")\n","        plt.show()\n","        return img\n","\n","    def save_model(self, dir):\n","        self.generator.save(dir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-10T22:43:10.358630Z","iopub.status.busy":"2022-02-10T22:43:10.358294Z","iopub.status.idle":"2022-02-10T22:43:10.366013Z","shell.execute_reply":"2022-02-10T22:43:10.365282Z","shell.execute_reply.started":"2022-02-10T22:43:10.358601Z"},"trusted":true},"outputs":[],"source":["image_w = 128\n","image_h = 128\n","image_size = (image_w, image_h)\n","channels = 3\n","image_shape = [image_w, image_h, channels]\n","\n","latent_dim = 128\n","batch_size = 16\n","l_rate = 0.0001\n","epochs = 300\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-10T22:43:10.370312Z","iopub.status.busy":"2022-02-10T22:43:10.369873Z","iopub.status.idle":"2022-02-10T22:43:10.379532Z","shell.execute_reply":"2022-02-10T22:43:10.378408Z","shell.execute_reply.started":"2022-02-10T22:43:10.370280Z"},"trusted":true},"outputs":[],"source":["data_dir = \"data\"\n","info = \"data/info/train.json\"\n","output_dir = \"output\"\n","models_dir = \"model\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-10T22:43:10.383317Z","iopub.status.busy":"2022-02-10T22:43:10.382897Z"},"trusted":true},"outputs":[],"source":["k_fold = 5\n","df = pd.read_json(info, orient=\"records\")\n","if not os.path.exists(output_dir):\n","    os.mkdir(output_dir)\n","if not os.path.exists(models_dir):\n","    os.mkdir(models_dir)\n","for i in range(k_fold, k_fold+1):\n","    dest = os.path.join(output_dir, str(i))\n","    if not os.path.exists(dest):\n","        create_dataset(df[df[\"group\"] != i], data_dir, dest)\n","    model = GAN(latent_dim, image_shape, batch_size, l_rate)\n","    dataset = get_dataset(dest, image_size, batch_size)\n","    model.train(dataset, epochs)\n","    model.save_model(os.path.join(models_dir, str(i)+\".h5\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.save_model(os.path.join(models_dir, str(i)+\".h5\"))\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
